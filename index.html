<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Awais Rauf


</title>
<meta name="description" content="Personal website. 
">

<!-- Open Graph -->

<meta property="og:site_name" content="Personal website. 
" />
<meta property="og:type" content="object" />
<meta property="og:title" content="" />
<meta property="og:url" content="https://awaisrauf.github.io//" />
<meta property="og:description" content="about" />
<meta property="og:image" content="" />


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<!-- <link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" /> -->
<link rel="stylesheet" href="/assets/css/github.css">

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ü§ñ</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-2PRP5MSJ91"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());

  gtag('config', 'G-2PRP5MSJ91');
</script>




    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
        <!-- Social Icons -->
        <div class="navbar-brand social">
          <a href="mailto:%69%61%77%61%69%73%72%61%75%66@%67%6D%61%69%6C.%63%6F%6D"><i class="fas fa-envelope"></i></a>

<a href="https://scholar.google.com/citations?user=bA-9t1cAAAAJ&hl" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>


<a href="https://github.com/awaisrauf" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
<a href="https://www.linkedin.com/in/deepawais" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a>




<a href="https://openreview.net/profile?id=~Muhammad_Awais2" target="_blank" title="Work"><i class="fas fa-briefcase"></i></a>






        </div>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">
              about
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>
          <!-- Blog Button -->
          <li class="nav-item">
            <a class="nav-link" href="https://awaisrauf.github.io/deepCuriosity/" target="_blank">
              üìùblog
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/research/">
                üî¨research
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/talks">
                üéôÔ∏ètalks
                
              </a>
          </li>
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/people/">
                üë•people
                
              </a>
          </li>
          
          
          
          
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-3">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     Awais Rauf
    </h1>
     <p class="desc"></p>
  </header>

  <article>
    
    <div class="profile float-right">
      
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/prof_pic.jpg">
      
      
        <div class="address">
          iawaisrauf at gmail dot com
        </div>
      
    </div>
    

    <div class="clearfix">
      <style>
.logos-container {
    display: flex;
    justify-content: center;
    flex-wrap: wrap;
    gap: 20px;
    margin-top: 20px;
}

.logo {
    width: 100%;
    max-width: 40px;
    height: auto;
    transition: all 0.3s ease-in-out;
}

/* Responsive adjustments */
@media (min-width: 540px) {
    .logo {
        max-width: 70px; 
    }
}

@media (min-width: 900px) {
    .logo {
        max-width: 80px;
    }
}

@media (max-width: 480px) {
    .logos-container {
        gap: 10px;
    }
    .logo {
        max-width: 45px;
    }
}
</style>

<p>I am a postdoc research associate at DERI, Queen Mary University of London, and an R&amp;D Advisor at BetterData, helping them develop tabular foundational models. I have been affiliated with,</p>
<div class="logos-container">
          <div class="company">
            <img src="assets/img/mbzuai.png" alt="" class="logo" />
        </div>
        <div class="company">
            <img src="assets/img/sonyai.jpg" alt="" class="logo" />
        </div>
        <div class="company">
            <img src="assets/img/huawei.png" alt="" class="logo" />
        </div>
      <div class="company">
            <img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSW-zl6llLzUFvqZSCz-lFtmypVHGfg9p0s8w&amp;s" alt="Company 4 Logo" class="logo" />
        </div>
    </div>

<!-- <ul>
 <li> RA at <a href="https://mbzuai.ac.ae/"> MBZUAI</a>, among <a href="https://csrankings.org/#/index?ai&vision&mlmining&nlp&world">top 15 AI research institutes</a></li>
  <li> RA at <a href="https://ai.sony/">Sony AI</a>, Japan
  </li>
  <li> RA at AI Theory Group of <a href="http://www.noahlab.com.hk/">Huawei's Noah Ark Lab</a> in Hong Kong
  </li>
  <li> Ph.D. Student and RA at <a href="https://sites.google.com/khu.ac.kr/mlvclab/">MLVC Lab</a> in South Korea
  </li> -->
<p><!-- <li> RA at <a href="http://www.spider.itu.edu.pk">SPIDER Lab</a> and a TA for five grad and undergrad courses, including <a href="https://awaisrauf.github.io/ee512/" class="muted-link">machine learning</a> at <a href="http://www.itu.edu.pk/" class="muted-link">ITU </a> </li> -->
  <!-- <li> Teaching Assistant for graduate and undergraduate courses, including <a href="https://awaisrauf.github.io/ee512/" class="muted-link">machine learning</a> -->
<!-- </ul> --></p>

<p>My research focuses on robust intelligent systems, cross-domain model adaptation, multi-modal foundational models, and the broader applications of AI with societal impact. I have published my research in leading AI and computer vision venues, such as NeurIPS (<a href="https://scholar.google.com.hk/citations?view_op=top_venues&amp;hl=en&amp;vq=eng_artificialintelligence">#1 in AI</a>), ICCV (<a href="https://scholar.google.com.hk/citations?view_op=top_venues&amp;hl=en&amp;vq=eng_computervisionpatternrecognition">#2 in CV</a>), ECCV (<a href="https://scholar.google.com.hk/citations?view_op=top_venues&amp;hl=en&amp;vq=eng_computervisionpatternrecognition">#3 in CV</a>), TPAMI (<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">IF=20.8</a>), TNNLS (<a href="https://cis.ieee.org/publications/t-neural-networks-and-learning-systems">IF=10.4</a>).</p>

<!-- <div class="logos-container">
        <img src="https://yt3.googleusercontent.com/_q_p7qZP7kmN9_F2Sf26zcUI23ShtGlFZNdGvdOp09ymubZI-q_32m2Sz-LRw5K2zfX2PF__=s900-c-k-c0x00ffffff-no-rj" alt="Company 1 Logo" class="logo">
        <img src="https://avatars.githubusercontent.com/u/12619994?s=200&v=4" alt="Company 2 Logo" class="logo">
        <img src="https://pbs.twimg.com/profile_images/1540236845400866817/snSslwU-_400x400.jpg" alt="Company 3 Logo" class="logo">
        <img src="https://lh6.googleusercontent.com/NwuzzRSYASBEdncLUKgnLhswIGbMgZopmw1--NSy62gXrHAoZEGUmkL6gxECJfJoXWrucXgG4gC2GSxUcobM1zk=w16383" alt="Company 4 Logo" class="logo">
    </div> -->

<p><!-- Details of my researcher is available here. I regularly contribute as a reviewer for top conferences such as NeurIPS, CVPR, ICLR, ICCV, and AISTATS. -->
 <!-- MICCAI ([#4 in Med. Imaging](https://scholar.google.com.hk/citations?view_op=top_venues&hl=en&vq=med_radiologymedicalimaging)), COLING ([#5 in Comp. Linguistics](https://scholar.google.com.hk/citations?view_op=top_venues&hl=en&vq=eng_computationallinguistics)), and WACV ([#9 in CV](https://scholar.google.com.hk/citations?view_op=top_venues&hl=en&vq=eng_computervisionpatternrecognition)). Details of my researcher is available here.   --></p>

<p>Additionally, I have won a <a href="https://propakistani.pk/2018/08/01/first-ever-election-prediction-contest-in-pakistan-concludes/">national-level ml competition</a>, received a travel grant to attend <a href="https://sites.google.com/view/2022-workshop-bridgingmathstcs">AustMS</a>, and secured <a href="https://developer.nvidia.com/academic_gpu_seeding">NVIDIA‚Äôs GPU grant</a> for research.</p>

<p>I enjoy <a href="https://www.goodreads.com/review/list/90419452-awais?page=1&amp;per_page=100&amp;print=true&amp;ref=nav_mybooks&amp;shelf=read&amp;utf8">üìñ reading</a>, <a href="https://awaisrauf.github.io/travel">üèõ traveling</a>, üßë‚Äçüíª coding and üë∑üèº‚Äç‚ôÇÔ∏è building stuff.</p>

    </div>

    
      <div class="news">
  <h2>Updates</h2>
  
    <div class="table-responsive">
      <table class="table table-sm table-borderless">
      
       
      <!-- limit: site.news_limit  -->
        
          <tr>
            <!-- &nbsp is for non-line-breaking space. -->
            <!-- <th> Mar,&nbsp2025 </th> -->
             <th> &nbsp2025: </th>
            <td>
              
                <a class="news-title" href="/news/news_20/">I am hosting Dr. Uzair Javaid, CEO of Betterdata, for an insightful session at QMUL.</a>
              
            </td>
          </tr>
          
       
      <!-- limit: site.news_limit  -->
        
          <tr>
            <!-- &nbsp is for non-line-breaking space. -->
            <!-- <th> Jan,&nbsp2025 </th> -->
             <th> &nbsp2025: </th>
            <td>
              
                Five papers have been accepted at <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">TPAMI</a>(IF&gt;24), <a href="https://wacv2025.thecvf.com/">WACV</a>, <a href="https://aclanthology.org/volumes/2025.coling-main/">COLING</a>, and <a href="https://biomedicalimaging.org/2025/">ISBI</a>. Congratulations to the co-authors.

              
            </td>
          </tr>
          
       
      <!-- limit: site.news_limit  -->
        
          <tr>
            <!-- &nbsp is for non-line-breaking space. -->
            <!-- <th> Jul,&nbsp2024 </th> -->
             <th> &nbsp2024: </th>
            <td>
              
                Four papers accepted at ECCV, MICCAI, ICASSP and SIVP. Congratulations to the co-authors.


              
            </td>
          </tr>
          
       
      <!-- limit: site.news_limit  -->
        
          <tr>
            <!-- &nbsp is for non-line-breaking space. -->
            <!-- <th> Sep,&nbsp2023 </th> -->
             <th> &nbsp2023: </th>
            <td>
              
                Successfully defended my Ph.D. thesis! Honored to receive the Outstanding Achievement Award and 2.5M KRW.

              
            </td>
          </tr>
          
       
      <!-- limit: site.news_limit  -->
        
       
      <!-- limit: site.news_limit  -->
        
          <tr>
            <!-- &nbsp is for non-line-breaking space. -->
            <!-- <th> Sep,&nbsp2022 </th> -->
             <th> &nbsp2022: </th>
            <td>
              
                One paper got accepted at the Neural Information Processing Systems - NeurIPS, 2022.


              
            </td>
          </tr>
          
       
      <!-- limit: site.news_limit  -->
        
       
      <!-- limit: site.news_limit  -->
        
          <tr>
            <!-- &nbsp is for non-line-breaking space. -->
            <!-- <th> May,&nbsp2022 </th> -->
             <th> &nbsp2022: </th>
            <td>
              
                Got travel funding to attend <a href="https://sites.google.com/view/2022-workshop-bridgingmathstcs">AustMS Workshop</a> on Bridging Maths and Computer Science.

              
            </td>
          </tr>
          
       
      <!-- limit: site.news_limit  -->
        
       
      <!-- limit: site.news_limit  -->
        
          <tr>
            <!-- &nbsp is for non-line-breaking space. -->
            <!-- <th> Aug,&nbsp2021 </th> -->
             <th> &nbsp2021: </th>
            <td>
              
                Two papers got accepted at NeurIPS and ICCV.

              
            </td>
          </tr>
          
       
      <!-- limit: site.news_limit  -->
        
       
      <!-- limit: site.news_limit  -->
        
          <tr>
            <!-- &nbsp is for non-line-breaking space. -->
            <!-- <th> Jun,&nbsp2020 </th> -->
             <th> &nbsp2020: </th>
            <td>
              
                One paper got accepted in IEEE Transactions on Neural Networks and Learning Systems - <a href="https://cis.ieee.org/publications/t-neural-networks-and-learning-systems">TNNLS</a>.

              
            </td>
          </tr>
          
       
      <!-- limit: site.news_limit  -->
        
       
      <!-- limit: site.news_limit  -->
        
       
      <!-- limit: site.news_limit  -->
        
       
      <!-- limit: site.news_limit  -->
        
       
      <!-- limit: site.news_limit  -->
        
       
      <!-- limit: site.news_limit  -->
        
          <tr>
            <!-- &nbsp is for non-line-breaking space. -->
            <!-- <th> Jul,&nbsp2018 </th> -->
             <th> &nbsp2018: </th>
            <td>
              
                Won <a href="https://propakistani.pk/2018/08/01/first-ever-election-prediction-contest-in-pakistan-concludes/">Election Prediction Contest</a> held by <a href="https://ignite.org.pk/">Ignite</a>,
 <a href="http://redbuffer.net/">Red Buffer</a>, <a href="http://deeplinks.pk/">DeepLinks</a> and <a href="https://twitter.com/CodeforPakistan/status/1024623283973578755">Code for Pakistan</a>, (<a href="http://awaisrauf.github.io/election_prediction">page</a>).

              
            </td>
          </tr>
          
       
      <!-- limit: site.news_limit  -->
        
          <tr>
            <!-- &nbsp is for non-line-breaking space. -->
            <!-- <th> Jul,&nbsp2017 </th> -->
             <th> &nbsp2017: </th>
            <td>
              
                <a href="https://www.nvidia.com">NVIDIA</a> has accepted our proposal for the <a href="https://developer.nvidia.com/academic_gpu_seeding">grant of Titan-X GPU</a> to support research.

              
            </td>
          </tr>
          
       
      <!-- limit: site.news_limit  -->
        
      
      </table>
    </div>
  
</div>

    

    
      <div class="publications">
  <h2>Selected Publications</h2>
  <ol class="bibliography"><li><!-- <style>
.paper-image {
  float: left;
  margin-top: 5px;
  margin-right: 10px; /* Space between image and text */
  width: 80; /* Adjust size as needed */
  height: auto;
}
</style> -->

<div class="row">

  <div class="col-sm-3 abbr">
  
    
    <abbr class="badge text-bg-secondary"><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34" target="_blank">TPAMI</a></abbr>
    
  
  <img class="paper-image" src="/assets/img/publication_preview/tpami23_preview.png" alt="Paper Image">

  </div>


  <div id="Awais.arXiv24" class="col-sm-9">
    
      <div class="title">Foundational Models Defining a New Era in Vision: A Survey and Outlook</div>
      <!-- <span class="robustness">Privacy & Security</span> -->
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Awais, Muhammad</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Muzammal, Naseer,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Khan, Salman,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Anwer, Rao,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Cholakkal, Hisham,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Shah, Mubarak,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yang, Ming-Hsuan,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Khan, Fahad Shahbaz
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>
      
      
        2024
      
      </div>
    

    <div class="links">
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2307.13721.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/awaisrauf/Awesome-CV-Foundational-Models" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><!-- <style>
.paper-image {
  float: left;
  margin-top: 5px;
  margin-right: 10px; /* Space between image and text */
  width: 80; /* Adjust size as needed */
  height: auto;
}
</style> -->

<div class="row">

  <div class="col-sm-3 abbr">
  
    
    <abbr class="badge text-bg-secondary"><a href="http://arxiv.org" target="_blank">arXiv</a></abbr>
    
  
  <img class="paper-image" src="/assets/img/publication_preview/frod.png" alt="Paper Image">

  </div>


  <div id="Awais.arXiv" class="col-sm-9">
    
      <div class="title">Leveraging Pre-trained Models for Efficient Cross-task Robustness Transfer for Object Detection</div>
      <!-- <span class="robustness">Privacy & Security</span> -->
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Awais, Muhammad</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Weiming, Zhuang,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Lingjuan, Lyu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Sung-Ho, Bae
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Under Review (arXiv)</em>
      
      
        2024
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2308.01888" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Object detection is an important task in computer vision and plays an essential role in many security-critical systems. Despite significant recent advancements, state-of-the-art object detection models remain vulnerable to small adversarial perturbations, which malicious actors can exploit to manipulate the model‚Äôs output. Unlike classification, the robustness of object detection models has received relatively limited attention, with most existing work focusing on adapting adversarial training. In this paper, we take a different approach, leveraging classification-based robust models to enhance the robustness of object detection. However, directly incorporating a classification-based robust backbone into object detection training can result in catastrophic forgetting of robust features. To address this issue, we propose efficient modifications to the robust backbone, enabling adversarial robustness with standard training. Additionally, we introduce a theoretically grounded, efficient two-phase training strategy that achieves state-of-the-art robustness. Our method first trains on normal examples while preserving robust features in the backbone, followed by adversarial training on single-step adversarial examples in the final stages of training. Our approach achieves state-of-the-art robustness and significantly improves efficiency compared to traditional adversarial training methods. Extensive experiments on the MS-COCO and Pascal VOC datasets validate the effectiveness of our proposed approach.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><!-- <style>
.paper-image {
  float: left;
  margin-top: 5px;
  margin-right: 10px; /* Space between image and text */
  width: 80; /* Adjust size as needed */
  height: auto;
}
</style> -->

<div class="row">

  <div class="col-sm-3 abbr">
  
    
    <abbr class="badge text-bg-secondary"><a href="https://eccv2024.ecva.net" target="_blank">ECCV</a></abbr>
    
  
  <img class="paper-image" src="/assets/img/publication_preview/eccv24.png" alt="Paper Image">

  </div>


  <div id="Kumar.eccv24" class="col-sm-9">
    
      <div class="title">Efficient 3D-Aware Facial Image Editing via Attribute-Specific Prompt Learning</div>
      <!-- <span class="robustness">Privacy & Security</span> -->
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Kumar, Amandeep*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Awais, Muhammad*</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Narayan, Sanath,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Cholakkal, Hisham,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Khan, Salman,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Anwer, Rao
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>European Conference on Computer Vision</em>
      
      
        2024
      
      </div>
    

    <div class="links">
    
    
    
    
      <a href="https://awaisrauf.github.io/3d_face_editing" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
    
    
    
    
    
      
      <a href="https://docs.google.com/presentation/d/11075T-pBxNIbvKGALQkKaRAQpNtmYbp82qx6zvAvu8g/edit#slide=id.g2d37125c826_1_15" class="btn btn-sm z-depth-0" role="button" target="_blank">Slides</a>
      
    
    
    
    </div>

    <!-- Hidden abstract block -->
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><!-- <style>
.paper-image {
  float: left;
  margin-top: 5px;
  margin-right: 10px; /* Space between image and text */
  width: 80; /* Adjust size as needed */
  height: auto;
}
</style> -->

<div class="row">

  <div class="col-sm-3 abbr">
  
    
    <abbr class="badge text-bg-secondary"><a href="https://miccai.org" target="_blank">MICCAI</a></abbr>
    
  
  <img class="paper-image" src="/assets/img/publication_preview/miccai24.png" alt="Paper Image">

  </div>


  <div id="hanif2024baple" class="col-sm-9">
    
      <div class="title">BAPLe: Backdoor Attacks on Medical Foundational Models using Prompt Learning</div>
      <!-- <span class="robustness">Privacy & Security</span> -->
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Hanif, Asif,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Shamshad, Fahad,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Awais, Muhammad</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Naseer, Muzammal,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Khan, Fahad Shahbaz,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Nandakumar, Karthik,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Khan, Salman,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Anwer, Rao
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
        2024
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      <a href="https://asif-hanif.github.io/baple/" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
      
      <a href="https://papers.miccai.org/miccai-2024/paper/3117_paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/asif-hanif/baple" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Medical foundation models are gaining prominence in the medical community for their ability to derive general representations from extensive collections of medical image-text pairs. Recent research indicates that these models are susceptible to backdoor attacks, which allow them to classify clean images accurately but fail when specific triggers are introduced. However, traditional backdoor attacks necessitate a considerable amount of additional data to maliciously pre-train a model. This requirement is often impractical in medical imaging applications due to the usual scarcity of data. Inspired by the latest developments in learnable prompts, this work introduces a method to embed a backdoor into the medical foundation model during the prompt learning phase. By incorporating learnable prompts within the text encoder and introducing imperceptible learnable noise trigger to the input images, we exploit the full capabilities of the medical foundation models (Med-FM). Our method, BAPLe, requires only a minimal subset of data to adjust the noise trigger and the text prompts for downstream tasks, enabling the creation of an effective backdoor attack. Through extensive experiments with four medical foundation models, each pre-trained on different modalities and evaluated across six downstream datasets, we demonstrate the efficacy of our approach. BAPLe achieves a high backdoor success rate across all models and datasets, outperforming the baseline backdoor attack methods. Our work highlights the vulnerability of Med-FMs towards backdoor attacks and strives to promote the safe adoption of Med-FMs before their deployment in real-world applications. We believe that our work will help the medical community understand the potential risks associated with deploying Med-FMs and encourage the development of robust and secure models.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><!-- <style>
.paper-image {
  float: left;
  margin-top: 5px;
  margin-right: 10px; /* Space between image and text */
  width: 80; /* Adjust size as needed */
  height: auto;
}
</style> -->

<div class="row">

  <div class="col-sm-3 abbr">
  
    
    <abbr class="badge text-bg-secondary"><a href="https://neurips.cc" target="_blank">NeurIPS</a></abbr>
    
  
  <img class="paper-image" src="/assets/img/publication_preview/ZooD.png" alt="Paper Image">

  </div>


  <div id="Awais.NeurIPS22" class="col-sm-9">
    
      <div class="title">ZooD: Exploiting model zoo for out-of-distribution generalization</div>
      <!-- <span class="robustness">Privacy & Security</span> -->
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Qishi, Dong*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Awais, Muhammad*</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Zhou, Fengwei,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Xie, Chuanlong,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Hu, Tianyang,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yang, Yongxin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Sung-Ho, Bae,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Li, Zhenguo
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Conference on Neural Information Processing Systems (NeurIPS)</em>
      
      
        2022
      
      </div>
    

    <div class="links">
    
    
    
    
    
      
      <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/cd305fdee96836d5cc1de94577d71b61-Paper-Conference.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
      
      <a href="https://slideslive.com/38991232/zood-exploiting-model-zoo-for-outofdistribution-generalization?ref=speaker-66115" class="btn btn-sm z-depth-0" role="button" target="_blank">Video</a>
      
    
    
    </div>

    <!-- Hidden abstract block -->
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><!-- <style>
.paper-image {
  float: left;
  margin-top: 5px;
  margin-right: 10px; /* Space between image and text */
  width: 80; /* Adjust size as needed */
  height: auto;
}
</style> -->

<div class="row">

  <div class="col-sm-3 abbr">
  
    
    <abbr class="badge text-bg-secondary"><a href="https://neurips.cc" target="_blank">NeurIPS</a></abbr>
    
  
  <img class="paper-image" src="/assets/img/publication_preview/mixacm.png" alt="Paper Image">

  </div>


  <div id="Awais.NeurIPS" class="col-sm-9">
    
      <div class="title">MixACM: Mixup-Based Robustness Transfer via Distillation of Activated Channel Maps</div>
      <!-- <span class="robustness">Privacy & Security</span> -->
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Awais, Muhammad*</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Fengwei, Zhou*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Chuanlong, Xie*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Jiawei, Li,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Sung-Ho, Bae,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Zhenguo, Li
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Conference on Neural Information Processing Systems (NeurIPS)</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://proceedings.neurips.cc/paper_files/paper/2021/file/240c945bb72980130446fc2b40fbb8e0-Paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
      
      <a href="https://neurips.cc/media/neurips-2021/Slides/27536.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
      
    
    
    
      
      <a href="https://slideslive.com/38967656/mixacm-mixupbased-robustness-transfer-via-distillation-of-activated-channel-maps" class="btn btn-sm z-depth-0" role="button" target="_blank">Video</a>
      
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Deep neural networks are susceptible to adversarially crafted, small and imperceptible changes in the natural inputs. The most effective defense mechanism against these examples is adversarial training which constructs adversarial examples during training by iterative maximization of loss. The model is then trained to minimize the loss on these constructed examples. This min-max optimization requires more data, larger capacity models, and additional computing resources. It also degrades the standard generalization performance of a model. Can we achieve robustness more efficiently? In this work, we explore this question from the perspective of knowledge transfer. First, we theoretically show the transferability of robustness from an adversarially trained teacher model to a student model with the help of mixup augmentation. Second, we propose a novel robustness transfer method called Mixup-Based Activated Channel Maps (MixACM) Transfer. MixACM transfers robustness from a robust teacher to a student by matching activated channel maps generated without expensive adversarial perturbations. Finally, extensive experiments on multiple datasets and different learning scenarios show our method can transfer robustness while also improving generalization on natural images.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><!-- <style>
.paper-image {
  float: left;
  margin-top: 5px;
  margin-right: 10px; /* Space between image and text */
  width: 80; /* Adjust size as needed */
  height: auto;
}
</style> -->

<div class="row">

  <div class="col-sm-3 abbr">
  
    
    <abbr class="badge text-bg-secondary"><a href="https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings" target="_blank">ICCV</a></abbr>
    
  
  <img class="paper-image" src="/assets/img/publication_preview/iccv22.png" alt="Paper Image">

  </div>


  <div id="Awais.ICCV21" class="col-sm-9">
    
      <div class="title">Adversarial Robustness for Unsupervised Domain Adaptation</div>
      <!-- <span class="robustness">Privacy & Security</span> -->
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Awais, Muhammad</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Fengwei, Zhou,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Hang, Xu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Lanqing, Hong,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Ping, Luo,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Sung-Ho, Bae,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Zhenguo, Li
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Internatioanl Conference on Computer Vision (ICCV)</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      <a href="http://awaisrauf.github.io/robust_uda" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
      
      <a href="https://arxiv.org/pdf/2109.00946.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Extensive Unsupervised Domain Adaptation (UDA) studies have shown great success in practice by learning transferable representations across a labeled source domain and an unlabeled target domain with deep models. However, previous works focus on improving the generalization ability of UDA models on clean examples without considering the adversarial robustness, which is crucial in real-world applications. Conventional adversarial training methods are not suitable for the adversarial robustness on the unlabeled target domain of UDA since they train models with adversarial examples generated by the supervised loss function. In this work, we leverage intermediate representations learned by multiple robust ImageNet models to improve the robustness of UDA models. Our method works by aligning the features of the UDA model with the robust features learned by ImageNet pre-trained models along with domain adaptation training. It utilizes both labeled and unlabeled domains and instills robustness without any adversarial intervention or label requirement during domain adaptation training. Experimental results show that our method significantly improves adversarial robustness compared to the baseline while keeping clean accuracy on various UDA benchmarks.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><!-- <style>
.paper-image {
  float: left;
  margin-top: 5px;
  margin-right: 10px; /* Space between image and text */
  width: 80; /* Adjust size as needed */
  height: auto;
}
</style> -->

<div class="row">

  <div class="col-sm-3 abbr">
  
    
    <abbr class="badge text-bg-secondary"><a href="https://cis.ieee.org/publications/t-neural-networks-and-learning-systems" target="_blank">TNNLS</a></abbr>
    
  
  <img class="paper-image" src="/assets/img/publication_preview/tnnls20.png" alt="Paper Image">

  </div>


  <div id="Awais.TNNLS20" class="col-sm-9">
    
      <div class="title">Revisiting Internal Covariant Shift for Batch Normalization</div>
      <!-- <span class="robustness">Privacy & Security</span> -->
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Awais, Muhammad</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Md. Tauhid Bin, Iqbal,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Sung-Ho, Bae
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>IEEE Transactions on Neural Networks and Learning Systems (TNNLS)</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      <a href="https://ieeexplore.ieee.org/document/9238401" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Despite the success of Batch Normalization (BatchNorm) and a plethora of its variants, the exact reasons for its success are still shady. The original BatchNorm paper explained it as a mechanism that reduces the Internal Covariate Shift (ICS), i.e., the distribution shifts in the input of the layers during training. Recently, some papers manifested skepticism on this hypothesis and provided \textitalternative explanations for the success of BatchNorm, such as the applicability of very high learning rates and the ability to smooth the landscape in optimization. In this work, we counter this  textit alternative arguments by demonstrating the importance of reduction in ICS following an empirical approach. We demonstrated various ways to achieve the above-mentioned alternative properties without any performance boost. In this light, we explored the importance of different BatchNorm parameters (i.e., batch statistics, affine transformation parameters) by visualizing their effectiveness in the performance and analyzed their connections with ICS. Afterward, we showed a different normalization scheme that fulfills all alternative explanations except reduction in ICS. Despite having all the alternative properties, we observed its poor performance, which nullifies the alternative claims rather signifies the importance of the ICS reduction. We performed comprehensive experiments on many variants of BatchNorm, finding that all of them similarly reduce ICS.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li></ol>
</div>

    

    
  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="sticky-bottom mt-5">
  <div class="container">
    &copy; Copyright 2025 Awais  Rauf.
    
    
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
